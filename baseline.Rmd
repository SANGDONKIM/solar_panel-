---
title: "solar panel data analysis"
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 5
    fig_height: 4
    theme: cosmo
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center")
```

# Preparations (준비작업) {.tabset .tabset-fade}

## Libraries

```{r load_lib, message=FALSE, warning=FALSE, results='hide'}
library(tidyverse)
library(tidymodels)
library(lubridate)
library(data.table)
library(skimr)
library(tibble)
library(tibbletime)
library(timetk)
library(imputeTS)
library(anomalize)
library(forecast)
library(GGally)
library(modeltime)
library(gt)
library(timetk)
library(lubridate)
library(janitor)
library(tidyquant)
library(modeltime.resample)
library(modeltime.ensemble)

# Visualization
library(ggthemes)
library(ggsci)
library(viridis)
library(ggExtra)


theme_set(theme_bw())
```

## Data load {.tabset .tabset-fade}

```{r}
file_path <- "./data/"
files <- list.files(file_path)
files
```

```{r, message=FALSE}
rdata1 <- fread(file.path(file_path, "rdata1.csv"))
rdata2 <- fread(file.path(file_path, "rdata2.csv"))

```

# Data overview (데이터 기본정보) {.tabset .tabset-fade}

## train data

```{r}
glimpse(rdata1)
skim(rdata1)

glimpse(rdata2)
skim(rdata2)

```

# 분석 프로세스

1.  울산 지역의 태양열 발전량에 대한 univariate time series model을 구축
2.  울산 지역의 일사량에 대한 univariate time series model 구축
3.  울산 지역 기온에 대한 univariate time series model 구축
4.  울산 지역의 일사량, 기온을 이용해서 multivariate time series model 구축



# 데이터 전처리 {.tabset .tabset-fade}

## change data type

```{r}
rdata2 %>%
    dplyr::select(time, ulsan) %>% 
    mutate(time = ymd_hms(time)) %>% 
    rename(date = time, value = ulsan) %>%  
    dplyr::filter(between(date, ymd('2015-02-01'), ymd('2021-02-01'))) -> ulsan

```

## generate future frame
```{r}
future_tbl <- 
    future_frame(.data = ulsan, .date_var = date, .length_out = '1 month') %>% 
    mutate(value = NA, 
           value = as.integer(NA))

```

## standardization

```{r}
ulsan %>% 
    mutate(value = standardize_vec(value)) -> ulsan

#Standardization Parameters
#mean: 66.3113618827161
#standard deviation: 104.187665325827
```



## data split

```{r}
splits <- time_series_split(ulsan, assess = "1 month", cumulative = TRUE)
```

# Feature engineering {.tabset .tabset-fade}

```{r}
ulsan_recipe <- 
    recipe(value~., data = training(splits)) %>% 
    step_timeseries_signature(date) %>% 
    step_rm(matches("(iso)|(xts)|(quarter)|(year)|(month)|(qday)|(diff)")) %>% 
    #step_lag(value, lag = 1:6) %>% 
    step_normalize(matches("(index.num)|(yday)")) %>% 
    step_dummy(all_nominal(), one_hot = TRUE) %>% 
    step_interact(~ matches("am.pm") * matches("wday.lbl")) %>% 
    step_fourier(date, period = c(24, 48, 76), K=2)
    
ulsan_recipe %>% prep() %>% juice() %>% head()

```

# time series workflow {.tabset .tabset-fade}

## Model fitting

```{r}
model_fit_prophet <- prophet_reg(seasonality_daily = TRUE) %>%
    set_engine(engine = "prophet") 

wflw_fit_prophet <- workflow() %>% 
    add_recipe(ulsan_recipe) %>%
    add_model(model_fit_prophet) %>%
    fit(training(splits)) 


model_fit_prophet_xgboost <- prophet_boost(seasonality_daily = TRUE) %>%
    set_engine(engine = "prophet_xgboost")

wflw_fit_prophet_boost <- workflow() %>% 
    add_recipe(ulsan_recipe) %>%
    add_model(model_fit_prophet_xgboost) %>% 
    fit(training(splits))


model_fit_nnetar <- nnetar_reg() %>% 
    set_engine('nnetar')

wflw_fit_nnetar <- workflow() %>% 
    add_recipe(ulsan_recipe) %>% 
    add_model(model_fit_nnetar) %>% 
    fit(training(splits))


# 6시간 이상 소요됨..
# model_fit_arima_boost <- modeltime::arima_boost() %>%
#     parsnip::set_engine("auto_arima_xgboost")
# 
# 
# wflw_fit_arima_boost <- workflow() %>% 
#     add_recipe(ulsan_recipe) %>% 
#     add_model(model_fit_arima_boost) %>% 
#     fit(training(splits))    

```

## Model table

```{r}
model_tbl <- modeltime_table(
  wflw_fit_prophet, 
  wflw_fit_prophet_boost, 
  wflw_fit_nnetar
)
```

## calibration

```{r}
calibration_tbl <- model_tbl %>% 
  modeltime_calibrate(new_data = testing(splits))
```

```{r}
calibration_tbl %>% 
    modeltime_accuracy(testing(splits)) %>%
    table_modeltime_accuracy(.interactive = FALSE)
```

## refit

```{r}
refit_tbl <- calibration_tbl %>% 
    modeltime_refit(data = ulsan) 
```

## forecast

```{r}
refit_tbl %>% 
    modeltime_forecast(
        new_data = future_tbl, 
        actual_data = ulsan 
    ) %>% 
    mutate(across(.value, .fns = ~ standardize_inv_vec(
        x = .,
        mean = 66.3113618827161,
        sd = 104.187665325827
    ))) -> result_tbl


#mean: 66.3113618827161
#standard deviation: 104.187665325827
    
```

# Ensemble workflow {.tabset .tabset-fade}

## Make an ensemble average

```{r}
# model_tbl <- modeltime_table(
#   wflw_fit_prophet, 
#   wflw_fit_prophet_boost, 
#   wflw_fit_nnetar
# )

ensemble_average_fit <- model_tbl %>% 
    ensemble_average(type = 'mean') %>% 
    modeltime_table() %>% 
    modeltime_calibrate(testing(splits))

ensemble_average_fit %>% 
    modeltime_accuracy(testing(splits)) %>%
    table_modeltime_accuracy(.interactive = FALSE) 

```

## Make an weighted ensemble

ensemble_weighted(object, loadings, scale_loadings = TRUE)

-   object : A Modeltime Table
-   loadings : A vector of weights corresponding to the loadings
-   scale_loadings : If TRUE, divides by the sum of the loadings to proportionally weight the submodels.

```{r}
ensemble_weighted_fit <- model_tbl %>% 
    ensemble_weighted(loadings = c(1 ,8, 1), scale_loadings = TRUE) %>% 
    modeltime_table() %>% 
    modeltime_calibrate(testing(splits))

ensemble_weighted_fit %>% 
    modeltime_accuracy(testing(splits)) %>%
    table_modeltime_accuracy(.interactive = FALSE) 

ensemble_weighted_fit  %>%
    modeltime_forecast(
        new_data    = testing(splits),
        actual_data = ulsan
    ) %>%
    plot_modeltime_forecast(.interactive =FALSE)

```


## Refit on Full Data & Forecast Future
```{r}

refit_ensemble_weighted_fit <- ensemble_weighted_fit %>% 
    modeltime_refit(ulsan)


refit_ensemble_weighted_fit %>% 
    modeltime_forecast(
        new_data = future_tbl,
        actual_data = ulsan
    ) %>% mutate(across(.value, .fns = ~ standardize_inv_vec(
        x = .,
        mean = 66.3113618827161,
        sd = 104.187665325827
    ))) -> ensemble_result 


```





```{r}
submission <- fread(file.path(file_path, "sample_submission.csv"))
result_tbl <- fread('result_tbl.csv')
result_dangjin_tbl <- fread('result_dangjin_tbl.csv')
result_floating_tbl <- fread('result_floating_tbl.csv')
result_warehouse_tbl <- fread('result_warehouse_tbl.csv')

a <- result_tbl %>% # ulsan
  filter(between(.index , ymd('2021-02-01'), ymd('2021-03-01'))) %>% 
  filter(.model_desc == 'NNAR(1,1,10)[24]', .key == 'prediction') %>% 
  select(.index, .value)

b <- result_dangjin_tbl %>% # dangjin  
  filter(between(.index , ymd('2021-02-01'), ymd('2021-03-01'))) %>% 
  filter(.model_desc == 'NNAR(1,1,10)[24]', .key == 'prediction') %>% 
  select(.index, .value)

c <- result_floating_tbl %>% # floating
  filter(between(.index , ymd('2021-02-01'), ymd('2021-03-01'))) %>% 
  filter(.model_desc == 'NNAR(1,1,10)[24]', .key == 'prediction') %>% 
  select(.index, .value)

d <- result_warehouse_tbl %>% # warehouse
  filter(between(.index , ymd('2021-02-01'), ymd('2021-03-01'))) %>% 
  filter(.model_desc == 'NNAR(1,1,10)[24]', .key == 'prediction') %>% 
  select(.index, .value)

submission$ulsan[1:672] <- a$.value
submission$dangjin[1:672] <- b$.value
submission$dangjin_floating[1:672] <- c$.value
submission$dangjin_warehouse[1:672] <- d$.value

write.csv(submission, 'nnetar1.csv', row.names = F)
  
```

# model ensemble로 적합하기 + test 결과 확인 + 울산 외 지역 채워넣기
